# ---------------------------------------------------------------------------
# $Id$
# ---------------------------------------------------------------------------

import re
import time
import urlparse

from classes.Common import *
from classes.Constants import *
from classes.Plugin import *

# ---------------------------------------------------------------------------

SCRAPE_TIMER = 'SCRAPE_TIMER'
RSS_TIMER = 'RSS_TIMER'

SELECT_QUERY = "SELECT url, description FROM torrents WHERE url IN (%s) OR description IN (%s)"
RECENT_QUERY = "SELECT added, url, description FROM torrents ORDER BY added DESC LIMIT 20"
INSERT_QUERY = "INSERT INTO torrents (added, url, description) VALUES (%s,%s,%s)"

# ---------------------------------------------------------------------------

class TorrentScraper(Plugin):
	def setup(self):
		self.rehash()
	
	def rehash(self):
		# Easy way to get general options
		self.SetupOptions('TorrentScraper')
		
		# Now get our URLs
		self.URLs = {}
		for option in self.Config.options('TorrentScraper-URLs'):
			self.URLs[self.Config.get('TorrentScraper-URLs', option)] = 0
	
	def register(self):
		self.setTimedEvent(SCRAPE_TIMER, int(self.Options['request_interval']), None)
		if self.Options['rss_path']:
			self.setTimedEvent(RSS_TIMER, int(self.Options['rss_interval']) * 60, None)
		
		self.registerEvents()
	
	# -----------------------------------------------------------------------
	# Get some URLs that haven't been checked recently
	def _trigger_SCRAPE_TIMER(self, trigger):
		interval = int(self.Options['scrape_interval']) * 60
		now = time.time()
		
		ready = [k for k, v in self.URLs.items() if now - v > interval]
		if ready:
			self.URLs[ready[0]] = now
			self.urlRequest(trigger, self.__Parse_Page, ready[0])
	
	def _trigger_RSS_TIMER(self, trigger):
		self.dbQuery(trigger, self.__Generate_RSS, RECENT_QUERY)
	
	# -----------------------------------------------------------------------
	# Do some page parsing!
	def __Parse_Page(self, trigger, resp):
		# Find all of our URLs
		chunks = FindChunks(resp.data, '<a ', '</a>')
		if not chunks:
			#self.sendReply(trigger, 'Page parsing failed: links.')
			self.putlog(LOG_WARNING, "Page parsing failed: links.")
			return
		
		# See if any are talking about torrents
		items = {}
		now = int(time.time())
		
		for chunk in chunks:
			# Find the URL
			href = FindChunk(chunk, 'href="', '"')
			if not href or href.find('.torrent') < 0:
				continue
			
			# Build the new URL
			newurl = urlparse.urljoin(resp.url, href)
			if newurl in items:
				continue
			
			# Get some text to describe it
			bits = chunk.split('>', 1)
			if len(bits) != 2:
				continue
			
			lines = StripHTML(bits[1])
			if len(lines) != 1:
				continue
			
			# Keep it for a bit
			items[newurl] = (now, newurl, lines[0])
		
		# If we found nothing, bug out
		if items == {}:
			tolog = "Found no torrents at %s!" % resp.url
			self.putlog(LOG_WARNING, tolog)
			return
		
		# Switch back to a list
		items = items.values()
		items.sort()
		
		# Build our query
		trigger.items = items
		
		args = [item[1] for item in items] + [item[2] for item in items]
		querybit = ', '.join(['%s'] * len(items))
		
		query = SELECT_QUERY % (querybit, querybit)
		
		# And execute it
		self.dbQuery(trigger, self.__DB_Check, query, *args)
	
	# -----------------------------------------------------------------------
	# Handle the check reply
	def __DB_Check(self, trigger, result):
		# Error!
		if result is None:
			return
		
		items = trigger.items
		del trigger.items
		
		# We don't need to add any that are already in the database
		for row in result:
			lurl = row['url'].lower()
			ldesc = row['description'].lower()
			items = [a for a in items if a[1].lower() != lurl and a[2].lower() != ldesc]
		
		# If we don't have any new items, go home now
		if len(items) == 0:
			return
		
		# Start adding the items to our database
		for item in items:
			self.dbQuery(trigger, None, INSERT_QUERY, *item)
	
	# -----------------------------------------------------------------------
	# Generate a simple RSS feed with our findings
	def __Generate_RSS(self, trigger, result):
		if result is None:
			self.putlog(LOG_WARNING, '__Generate_RSS: A DB error occurred!')
			return
		
		t1 = time.time()
		
		try:
			rssfile = open(self.Options['rss_path'], 'w')
		
		except Exception, msg:
			tolog = "Error opening %s for writing: %s" % (self.Options['rss_path'], msg)
			self.putlog(LOG_WARNING, tolog)
			return
		
		# RSS header
		builddate = ISODate(time.time())
		
		print >>rssfile, """<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0">
<channel>
<title>TorrentScraper</title>
<link>http://www.nowhere.com</link>
<description>An RSS feed generated by TorrentScraper</description>
<language>en-us</language>
<lastBuildDate>%s</lastBuildDate>
<generator>blamehangle</generator>
<ttl>%s</ttl>""" % (builddate, int(self.Options['rss_interval']) * 60)
		
		# Items!
		for row in result:
			lines = []
			lines.append('<item>')
			lines.append('<title>%s</title>' % row['description'])
			lines.append('<guid>%s</guid>' % QuoteURL(row['url']))
			lines.append('<pubDate>%s</pubDate>' % ISODate(row['added']))
			lines.append('</item>')
			print >>rssfile, '\n'.join(lines)
		
		# RSS footer
		print >>rssfile, """</channel>
</rss>"""
		
		rssfile.close()
		
		tolog = "RSS feed generated in %.2fs" % (time.time() - t1)
		self.putlog(LOG_ALWAYS, tolog)

# ---------------------------------------------------------------------------
# Return an ISOblah date string
def ISODate(t):
	return time.strftime("%a, %d %b %Y %H:%M:%S %Z", time.localtime(t))

# ---------------------------------------------------------------------------
